#!/usr/bin/env python
"""
åˆ†æColorHunté…è‰²æ–¹æ¡ˆé¡µé¢çš„HTMLç»“æ„
æ‰¾åˆ°æ ‡ç­¾ã€ç‚¹èµæ•°ã€æ—¥æœŸç­‰å…ƒç´ çš„å…·ä½“ä½ç½®
"""
import sys
import os
import requests
from bs4 import BeautifulSoup
import re

def analyze_html_structure():
    """åˆ†æHTMLç»“æ„"""
    print("ğŸ” åˆ†æColorHunté…è‰²æ–¹æ¡ˆé¡µé¢HTMLç»“æ„")
    print("=" * 60)
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1'
    }
    
    # æµ‹è¯•æˆªå›¾ä¸­çš„é…è‰²æ–¹æ¡ˆ
    url = 'https://colorhunt.co/palette/626f47a4b465f5ecd5f0bb78'
    
    print(f"ğŸ“‹ åˆ†æURL: {url}")
    print("-" * 40)
    
    try:
        response = requests.get(url, headers=headers, timeout=10)
        
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            
            print("âœ… é¡µé¢åŠ è½½æˆåŠŸ")
            print(f"é¡µé¢æ ‡é¢˜: {soup.title.text if soup.title else 'N/A'}")
            
            # ä¿å­˜å®Œæ•´HTMLç”¨äºåˆ†æ
            with open('palette_page.html', 'w', encoding='utf-8') as f:
                f.write(response.text)
            print("ğŸ“ å·²ä¿å­˜å®Œæ•´HTML: palette_page.html")
            
            # åˆ†æé¡µé¢ç»“æ„
            print("\nğŸ” é¡µé¢ç»“æ„åˆ†æ:")
            
            # æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½åŒ…å«æ ‡ç­¾çš„å…ƒç´ 
            print("\n1. æŸ¥æ‰¾æ‰€æœ‰é“¾æ¥:")
            links = soup.find_all('a')
            for i, link in enumerate(links[:20]):  # åªæ˜¾ç¤ºå‰20ä¸ª
                href = link.get('href', '')
                text = link.text.strip()
                if text:
                    print(f"   é“¾æ¥{i+1}: {text} -> {href}")
            
            print("\n2. æŸ¥æ‰¾æ‰€æœ‰classå±æ€§:")
            all_elements = soup.find_all(True)
            classes = set()
            for elem in all_elements:
                if elem.get('class'):
                    classes.update(elem.get('class'))
            
            for cls in sorted(classes):
                print(f"   class: {cls}")
            
            print("\n3. æŸ¥æ‰¾åŒ…å«æ•°å­—çš„æ–‡æœ¬ï¼ˆå¯èƒ½æ˜¯ç‚¹èµæ•°ï¼‰:")
            page_text = soup.get_text()
            number_matches = re.findall(r'\b\d+\b', page_text)
            for num in number_matches[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
                print(f"   æ•°å­—: {num}")
            
            print("\n4. æŸ¥æ‰¾æ—¶é—´ç›¸å…³çš„æ–‡æœ¬:")
            time_matches = re.findall(r'\b\d+\s+(hour|day|week|month|year)s?\b', page_text, re.IGNORECASE)
            for match in time_matches:
                print(f"   æ—¶é—´: {match}")
            
            print("\n5. æŸ¥æ‰¾metaæ ‡ç­¾:")
            meta_tags = soup.find_all('meta')
            for meta in meta_tags:
                name = meta.get('name', meta.get('property', ''))
                content = meta.get('content', '')
                if name and content:
                    print(f"   {name}: {content[:100]}...")
            
            print("\n6. æŸ¥æ‰¾scriptæ ‡ç­¾ä¸­çš„æ•°æ®:")
            scripts = soup.find_all('script')
            for i, script in enumerate(scripts):
                if script.string and ('tag' in script.string.lower() or 'like' in script.string.lower()):
                    print(f"   Script {i+1}: {script.string[:200]}...")
            
        else:
            print(f"âŒ è¯·æ±‚å¤±è´¥, çŠ¶æ€ç : {response.status_code}")
            
    except Exception as e:
        print(f"âŒ è¯·æ±‚å¼‚å¸¸: {e}")

def check_feed_api():
    """æ£€æŸ¥feed.php APIçš„å“åº”"""
    print("\n" + "=" * 60)
    print("ğŸ” æ£€æŸ¥feed.php APIå“åº”")
    print("=" * 60)
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
        'Accept': 'application/json, text/html, */*',
        'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
        'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8',
        'X-Requested-With': 'XMLHttpRequest',
        'Referer': 'https://colorhunt.co/'
    }
    
    # æµ‹è¯•ä¸åŒçš„APIå‚æ•°ï¼Œçœ‹çœ‹æ˜¯å¦æœ‰å…¶ä»–å­—æ®µ
    test_cases = [
        {'step': 0, 'sort': 'popular', 'tags': '', 'timeframe': '30'},
        {'step': 0, 'sort': 'new', 'tags': 'vintage', 'timeframe': ''},
        {'step': 0, 'sort': 'new', 'tags': 'nature', 'timeframe': ''}
    ]
    
    for i, post_data in enumerate(test_cases):
        print(f"\nğŸ“‹ æµ‹è¯•APIå‚æ•° {i+1}: {post_data}")
        
        try:
            response = requests.post(
                'https://colorhunt.co/php/feed.php', 
                headers=headers, 
                data=post_data,
                timeout=10
            )
            
            print(f"çŠ¶æ€ç : {response.status_code}")
            print(f"å“åº”é•¿åº¦: {len(response.text)}")
            print(f"å“åº”å†…å®¹: {response.text[:500]}...")
            
            if response.text.strip() == '[]':
                print("âš ï¸ APIè¿”å›ç©ºæ•°ç»„")
            elif response.text.strip() == '':
                print("âš ï¸ APIè¿”å›ç©ºå“åº”")
            
        except Exception as e:
            print(f"âŒ APIè¯·æ±‚å¼‚å¸¸: {e}")

def main():
    """ä¸»å‡½æ•°"""
    print("ColorHunté¡µé¢ç»“æ„åˆ†æ")
    print("=" * 60)
    
    try:
        analyze_html_structure()
        check_feed_api()
        
        print("\nğŸ‰ åˆ†æå®Œæˆï¼")
        print("ğŸ” æ£€æŸ¥ç”Ÿæˆçš„palette_page.htmlæ–‡ä»¶æŸ¥çœ‹å®Œæ•´é¡µé¢ç»“æ„ã€‚")
        
    except Exception as e:
        print(f"âŒ åˆ†æè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 