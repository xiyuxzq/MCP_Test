#!/usr/bin/env python
"""
æµ‹è¯•ä»ColorHunté…è‰²æ–¹æ¡ˆé¡µé¢æå–æ ‡ç­¾ä¿¡æ¯
åˆ†æç½‘é¡µHTMLç»“æ„ï¼Œè·å–çœŸå®çš„æ ‡ç­¾æ•°æ®
"""
import sys
import os
import json
import requests
from bs4 import BeautifulSoup
import re

def test_webpage_tags():
    """æµ‹è¯•ä»ç½‘é¡µæå–æ ‡ç­¾"""
    print("ğŸ” æµ‹è¯•ä»ColorHunté…è‰²æ–¹æ¡ˆé¡µé¢æå–æ ‡ç­¾")
    print("=" * 60)
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1'
    }
    
    # æµ‹è¯•å‡ ä¸ªä¸åŒçš„é…è‰²æ–¹æ¡ˆURL
    test_urls = [
        'https://colorhunt.co/palette/626f47a4b465f5ecd5f0bb78',  # ä»æˆªå›¾æ¨æµ‹çš„URL
        'https://colorhunt.co/palette/222831393e46948979dfd0b8',  # ä¹‹å‰æµ‹è¯•çš„popularé…è‰²
        'https://colorhunt.co/palette/ffe99affd586ffaaaaff9898'   # vintageé…è‰²
    ]
    
    for url in test_urls:
        print(f"\nğŸ“‹ æµ‹è¯•URL: {url}")
        print("-" * 40)
        
        try:
            response = requests.get(url, headers=headers, timeout=10)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # æå–é…è‰²æ–¹æ¡ˆä¿¡æ¯
                palette_info = extract_palette_info(soup, url)
                
                if palette_info:
                    print("âœ… æˆåŠŸæå–é…è‰²æ–¹æ¡ˆä¿¡æ¯:")
                    for key, value in palette_info.items():
                        print(f"   {key}: {value}")
                    
                    # ä¿å­˜æå–çš„æ•°æ®
                    palette_id = url.split('/')[-1]
                    filename = f"webpage_tags_{palette_id}.json"
                    with open(filename, 'w', encoding='utf-8') as f:
                        json.dump(palette_info, f, indent=2, ensure_ascii=False)
                    print(f"   ğŸ“ å·²ä¿å­˜: {filename}")
                else:
                    print("âŒ æœªèƒ½æå–åˆ°é…è‰²æ–¹æ¡ˆä¿¡æ¯")
                    
            else:
                print(f"âŒ è¯·æ±‚å¤±è´¥, çŠ¶æ€ç : {response.status_code}")
                
        except Exception as e:
            print(f"âŒ è¯·æ±‚å¼‚å¸¸: {e}")

def extract_palette_info(soup, url):
    """ä»ç½‘é¡µHTMLä¸­æå–é…è‰²æ–¹æ¡ˆä¿¡æ¯"""
    info = {
        'url': url,
        'palette_id': url.split('/')[-1],
        'colors': [],
        'tags': [],
        'likes': 0,
        'date': '',
        'title': '',
        'author': ''
    }
    
    try:
        # æå–æ ‡é¢˜
        title_elem = soup.find('title')
        if title_elem:
            info['title'] = title_elem.text.strip()
        
        # æå–é¢œè‰²ï¼ˆä»URLè§£æï¼‰
        palette_id = info['palette_id']
        if len(palette_id) == 24:
            for i in range(4):
                hex_color = f"#{palette_id[i*6:(i+1)*6].upper()}"
                info['colors'].append(hex_color)
        
        # æ–¹æ³•1: æŸ¥æ‰¾æ ‡ç­¾é“¾æ¥
        tag_links = soup.find_all('a', href=re.compile(r'/palettes/'))
        for link in tag_links:
            tag_text = link.text.strip()
            if tag_text and tag_text not in info['tags']:
                # è¿‡æ»¤æ‰ä¸€äº›éæ ‡ç­¾çš„é“¾æ¥
                if not any(x in tag_text.lower() for x in ['palette', 'color', 'hunt', 'home']):
                    info['tags'].append(tag_text)
        
        # æ–¹æ³•2: æŸ¥æ‰¾åŒ…å«æ ‡ç­¾çš„divæˆ–spanå…ƒç´ 
        tag_elements = soup.find_all(['div', 'span', 'a'], class_=re.compile(r'tag|label|category'))
        for elem in tag_elements:
            tag_text = elem.text.strip()
            if tag_text and tag_text not in info['tags']:
                if len(tag_text) < 20 and tag_text.isalpha():  # æ ‡ç­¾é€šå¸¸æ˜¯çŸ­çš„å•è¯
                    info['tags'].append(tag_text)
        
        # æ–¹æ³•3: æŸ¥æ‰¾é¡µé¢ä¸­çš„æ‰€æœ‰é“¾æ¥ï¼Œç­›é€‰å¯èƒ½çš„æ ‡ç­¾
        all_links = soup.find_all('a')
        for link in all_links:
            href = link.get('href', '')
            text = link.text.strip()
            
            # å¦‚æœé“¾æ¥æŒ‡å‘æ ‡ç­¾é¡µé¢ï¼Œä¸”æ–‡æœ¬æ˜¯å•ä¸ªè¯
            if '/palettes/' in href and text and len(text.split()) == 1:
                if text.lower() not in ['home', 'new', 'popular', 'random'] and text not in info['tags']:
                    info['tags'].append(text)
        
        # æå–ç‚¹èµæ•°
        like_patterns = [
            r'(\d+)\s*likes?',
            r'likes?\s*(\d+)',
            r'â¤ï¸\s*(\d+)',
            r'â™¥\s*(\d+)'
        ]
        
        page_text = soup.get_text()
        for pattern in like_patterns:
            match = re.search(pattern, page_text, re.IGNORECASE)
            if match:
                info['likes'] = int(match.group(1))
                break
        
        # æå–æ—¥æœŸ/æ—¶é—´
        time_patterns = [
            r'(\d+)\s+(hour|day|week|month|year)s?\s+ago',
            r'(\d+)\s+(hour|day|week|month|year)s?',
            r'(yesterday|today)',
            r'(\d{4}-\d{2}-\d{2})'
        ]
        
        for pattern in time_patterns:
            match = re.search(pattern, page_text, re.IGNORECASE)
            if match:
                info['date'] = match.group(0)
                break
        
        # æŸ¥æ‰¾metaæ ‡ç­¾ä¸­çš„ä¿¡æ¯
        meta_desc = soup.find('meta', {'name': 'description'})
        if meta_desc:
            desc = meta_desc.get('content', '')
            # ä»æè¿°ä¸­æå–å¯èƒ½çš„æ ‡ç­¾
            desc_words = re.findall(r'\b[a-zA-Z]+\b', desc)
            for word in desc_words:
                if len(word) > 2 and word.lower() not in ['color', 'palette', 'hunt', 'the', 'and', 'for']:
                    if word not in info['tags']:
                        info['tags'].append(word)
        
        # æ¸…ç†æ ‡ç­¾åˆ—è¡¨
        info['tags'] = [tag for tag in info['tags'] if len(tag) > 1 and len(tag) < 15]
        info['tags'] = list(set(info['tags']))  # å»é‡
        
        return info
        
    except Exception as e:
        print(f"âŒ æå–ä¿¡æ¯æ—¶å‡ºé”™: {e}")
        return None

def main():
    """ä¸»å‡½æ•°"""
    print("ColorHuntç½‘é¡µæ ‡ç­¾æå–æµ‹è¯•")
    print("=" * 60)
    
    try:
        test_webpage_tags()
        
        print("\nğŸ‰ æµ‹è¯•å®Œæˆï¼")
        print("ğŸ” æ£€æŸ¥ç”Ÿæˆçš„webpage_tags_*.jsonæ–‡ä»¶æŸ¥çœ‹æå–çš„æ ‡ç­¾æ•°æ®ã€‚")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 